{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishna/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Union, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from scipy.special import betaln, gammaln, digamma\n",
    "import requests\n",
    "from matplotlib.colors import Colormap\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ARRAY_OR_FLOAT = Union[np.ndarray, float]\n",
    "def multivariate_betaln(alphas: np.ndarray) -> ARRAY_OR_FLOAT:\n",
    "    if len(alphas) == 2:\n",
    "        return betaln(alphas[0], alphas[1])\n",
    "    else:\n",
    "        # see https://en.wikipedia.org/wiki/Beta_function#Multivariate_beta_function\n",
    "        return np.sum([gammaln(alpha) for alpha in alphas], axis=0) - gammaln(alphas.sum())\n",
    "\n",
    "\n",
    "def compute_log_p_data(\n",
    "        prior: np.ndarray,\n",
    "        k: Union[np.ndarray, float],\n",
    "        betaln_prior: float) -> Union[np.ndarray, float]:\n",
    "    # see https://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall06/reading/bernoulli.pdf, equation (42)\n",
    "    # which can be expressed as a fraction of beta functions\n",
    "    posterior = prior + k\n",
    "    return multivariate_betaln(posterior.T) - betaln_prior\n",
    "\n",
    "\n",
    "def d_log_multivariate_beta_d_alphas(alphas: np.ndarray) -> ARRAY_OR_FLOAT:\n",
    "    return digamma(alphas) - digamma(np.sum(alphas))\n",
    "\n",
    "\n",
    "def sigmoid(x: ARRAY_OR_FLOAT) -> ARRAY_OR_FLOAT:\n",
    "    # avoid underflow/overflow\n",
    "    LOW = -708\n",
    "    HIGH = 40\n",
    "\n",
    "    if np.isscalar(x):\n",
    "        return 1 if x > HIGH else 0 if x < LOW else 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        lt_40 = x <= HIGH\n",
    "        gt_m708 = x >= LOW\n",
    "        if np.all(lt_40) and np.all(gt_m708):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            result = np.zeros(x.shape)\n",
    "            result[~lt_40] = 1\n",
    "            valid = np.where(lt_40 & gt_m708)[0]\n",
    "            result[valid] = 1 / (1 + np.exp(-x[valid]))\n",
    "            return result\n",
    "\n",
    "\n",
    "def pick_proportional(relative_probabilities: np.ndarray):\n",
    "    # from page 161 of\n",
    "    # https://github.com/Grant6899/books/blob/master/%5BMark%20Joshi%5DQuant%20Job%20Interview%20Questions%20And%20Answers.pdf\n",
    "    p = relative_probabilities / np.sum(relative_probabilities)\n",
    "    u = np.random.uniform(0, 1)\n",
    "    i = 0\n",
    "    while u > p[i]:\n",
    "        u -= p[i]\n",
    "        i += 1\n",
    "\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adam optimizer settings (see https://arxiv.org/abs/1412.6980)\n",
    "BETA_1_ADAM = 0.9\n",
    "BETA_2_ADAM = 0.999\n",
    "EPS_ADAM = 1e-8\n",
    "\n",
    "\n",
    "# Note: The following three methods must adhere to the same node iteration order\n",
    "# in order to avoid weight vectors getting wrongly assigned to nodes:\n",
    "#\n",
    "# - _collect_weight_matrix\n",
    "# - _distribute_weight_matrix\n",
    "# - _compute_d_err_d_s_hat_and_collect_d_err_d_weights\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    A node in the adaptive Bayesian reticulum tree.\n",
    "    \"\"\"\n",
    "    level: int\n",
    "\n",
    "    left__child: Optional[Node] = None\n",
    "    right_child: Optional[Node] = None\n",
    "\n",
    "    weights: Optional[np.ndarray] = None\n",
    "\n",
    "    s: Optional[np.ndarray] = None\n",
    "    s_hat_left_: Optional[np.ndarray] = None\n",
    "    s_hat_right: Optional[np.ndarray] = None\n",
    "\n",
    "    k_left_: Optional[int] = None\n",
    "    k_right: Optional[int] = None\n",
    "\n",
    "    posterior_left_: Optional[np.ndarray] = None\n",
    "    posterior_right: Optional[np.ndarray] = None\n",
    "\n",
    "    log_p_data_left_: float = None\n",
    "    log_p_data_right: float = None\n",
    "\n",
    "    log_p_data_split = None\n",
    "    log_p_data_no_split = None\n",
    "\n",
    "    def try_fit(\n",
    "            self,\n",
    "            Xa: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "            prior: np.ndarray,\n",
    "            s_hat_parent: Optional[np.ndarray],\n",
    "            learning_rate_init: float,\n",
    "            n_gradient_descent_steps: Optional[int],\n",
    "            initial_relative_stiffness: Optional[float]) -> bool:\n",
    "        if s_hat_parent is None:\n",
    "            n_data = Xa.shape[0]\n",
    "            s_hat_parent = np.ones(n_data)\n",
    "\n",
    "        # 1. initialize weights (if required)\n",
    "        if self.weights is not None:\n",
    "            # use weights that are already set on this node\n",
    "            initial_weights = self.weights\n",
    "        else:\n",
    "            # initialize by choosing a random weight vector\n",
    "            initial_weights = self._initialize_weights(\n",
    "                Xa=Xa,\n",
    "                s_hat_parent=s_hat_parent,\n",
    "                initial_relative_stiffness=initial_relative_stiffness)\n",
    "\n",
    "        if initial_weights is not None:\n",
    "            self.weights = initial_weights\n",
    "        else:\n",
    "            # no data that we could split\n",
    "            return False\n",
    "\n",
    "        # 2. optimize weights of newly added node using gradient descent (i.e., local optimization)\n",
    "        self._perform_gradient_descent(\n",
    "            Xa=Xa,\n",
    "            y=y,\n",
    "            s_hat_parent=s_hat_parent,\n",
    "            learning_rate_init=learning_rate_init,\n",
    "            n_gradient_descent_steps=n_gradient_descent_steps,\n",
    "            prior=prior)\n",
    "\n",
    "        # 3. compute log-likelihoods of not splitting vs. splitting\n",
    "        log_p_data_no_split = self._compute_log_p_data_no_split(y, s_hat_parent, prior)\n",
    "        self.update_log_p_data_soft_split(Xa, y, s_hat_parent, prior, recursive=False)\n",
    "        log_p_data_split = self.log_p_data_left_ + self.log_p_data_right\n",
    "\n",
    "        # keep and complete\n",
    "        self.posterior_left_ = prior + self.k_left_\n",
    "        self.posterior_right = prior + self.k_right\n",
    "        self.log_p_data_split = log_p_data_split\n",
    "        self.log_p_data_no_split = log_p_data_no_split\n",
    "\n",
    "        return True\n",
    "\n",
    "    def try_prune(\n",
    "            self,\n",
    "            Xa: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "            prior: np.ndarray,\n",
    "            pruning_factor: float,\n",
    "            verbose: bool,\n",
    "            s_hat_parent: Optional[np.ndarray]=None) -> Tuple[bool, bool]:\n",
    "        any_child_pruned = False\n",
    "\n",
    "        # recursively try pruning children first because actual pruning must start at the leaf level\n",
    "        if self.left__child is not None:\n",
    "            left_child_pruned, any_child_of_left_child_pruned = self.left__child.try_prune(\n",
    "                Xa=Xa,\n",
    "                y=y,\n",
    "                prior=prior,\n",
    "                pruning_factor=pruning_factor,\n",
    "                verbose=verbose,\n",
    "                s_hat_parent=self.s_hat_left_)\n",
    "\n",
    "            if left_child_pruned:\n",
    "                self.left__child = None\n",
    "\n",
    "            any_child_pruned |= left_child_pruned\n",
    "            any_child_pruned |= any_child_of_left_child_pruned\n",
    "\n",
    "        if self.right_child is not None:\n",
    "            right_child_pruned, any_child_of_right_child_pruned = self.right_child.try_prune(\n",
    "                Xa=Xa,\n",
    "                y=y,\n",
    "                prior=prior,\n",
    "                pruning_factor=pruning_factor,\n",
    "                verbose=verbose,\n",
    "                s_hat_parent=self.s_hat_right)\n",
    "\n",
    "            if right_child_pruned:\n",
    "                self.right_child = None\n",
    "\n",
    "            any_child_pruned |= right_child_pruned\n",
    "            any_child_pruned |= any_child_of_right_child_pruned\n",
    "\n",
    "        # now try pruning this node if it has no children (either because it never had or because they just got pruned)\n",
    "        if self.left__child is None and self.right_child is None:\n",
    "            if s_hat_parent is None:\n",
    "                n_data = Xa.shape[0]\n",
    "                s_hat_parent = np.ones(n_data)\n",
    "\n",
    "            # check if this split is adding value\n",
    "            pruning_factor_for_level = pruning_factor**(self.level+1)\n",
    "            log_p_data_no_split = self._compute_log_p_data_no_split(y, s_hat_parent, prior)\n",
    "            log_p_data_split = self.log_p_data_left_ + self.log_p_data_right\n",
    "            if log_p_data_split <= log_p_data_no_split + np.log(pruning_factor_for_level):\n",
    "                # this split isn't adding value (anymore) -> prune\n",
    "                if verbose:\n",
    "                    print(f'Pruning node at level {self.level}')\n",
    "\n",
    "                return True, any_child_pruned\n",
    "\n",
    "        # check if this split is actually splitting data\n",
    "        activation = Xa @ self.weights\n",
    "        if np.all(activation < 0) or np.all(activation > 0):\n",
    "            # all data on one side -> not actually splitting anything\n",
    "            return True, any_child_pruned\n",
    "\n",
    "        return False, any_child_pruned\n",
    "\n",
    "    def update_probability(self, Xa: np.ndarray, p: np.ndarray) -> None:\n",
    "        if self.left__child is None:\n",
    "            p += self.s_hat_left_.reshape(-1, 1) @ self.predict_proba_leaf_left_().reshape(1, -1)\n",
    "        else:\n",
    "            self.left__child.update_probability(Xa, p)\n",
    "\n",
    "        if self.right_child is None:\n",
    "            p += self.s_hat_right.reshape(-1, 1) @ self.predict_proba_leaf_right().reshape(1, -1)\n",
    "        else:\n",
    "            self.right_child.update_probability(Xa, p)\n",
    "\n",
    "    def collect_terminal_nodes(self, is_left: bool, nodes: List[Node]=None) -> List[Node]:\n",
    "        if nodes is None:\n",
    "            nodes = []\n",
    "\n",
    "        if self.left__child:\n",
    "            self.left__child.collect_terminal_nodes(is_left, nodes)\n",
    "        elif is_left:\n",
    "            nodes.append(self)\n",
    "\n",
    "        if self.right_child:\n",
    "            self.right_child.collect_terminal_nodes(is_left, nodes)\n",
    "        elif not is_left:\n",
    "            nodes.append(self)\n",
    "\n",
    "        return nodes\n",
    "\n",
    "    @staticmethod\n",
    "    def _initialize_weights(\n",
    "            Xa: np.ndarray,\n",
    "            s_hat_parent: np.ndarray,\n",
    "            initial_relative_stiffness: float) -> Optional[np.ndarray]:\n",
    "        assert s_hat_parent.ndim == 1\n",
    "\n",
    "        n_dim = Xa.shape[1]-1  # ignore the augmented offset dimension\n",
    "\n",
    "        # remove data points that have a too low weight\n",
    "        keep_condition = s_hat_parent >= 0.5\n",
    "        Xa = Xa[keep_condition, :]\n",
    "        if len(Xa) < 2:\n",
    "            # no point in splitting data sets with less than two points\n",
    "            return None\n",
    "\n",
    "        # compute outlier-resistant data range and compute weight scaling\n",
    "        half_range = np.quantile(Xa[:, 1:], 0.75, axis=0) - np.quantile(Xa[:, 1:], 0.25, axis=0)\n",
    "        zeros = half_range == 0\n",
    "        if np.any(zeros):\n",
    "            # first fix for zero entries: use half of the full range\n",
    "            half_range[zeros] = (0.5 * (np.max(Xa[:, 1:], axis=0) - np.min(Xa[:, 1:], axis=0)))[zeros]\n",
    "\n",
    "        zeros = half_range == 0\n",
    "        if np.any(zeros):\n",
    "            # second fix for zero entries: use 1\n",
    "            half_range[zeros] = 1\n",
    "\n",
    "        sd = 1/half_range\n",
    "\n",
    "        # choose random initial weight appropriately scaled to the data\n",
    "        weights = np.random.normal(0, 1, n_dim)\n",
    "        weights /= np.linalg.norm(weights)\n",
    "        weights *= sd\n",
    "\n",
    "        # center the weight vector at the median of the data\n",
    "        median = np.median(Xa[:, 1:], axis=0)\n",
    "        weights = np.insert(weights, 0, 0)\n",
    "        weights[0] = -np.dot(median, weights[1:])\n",
    "\n",
    "        # apply initial stiffness\n",
    "        return weights * initial_relative_stiffness\n",
    "\n",
    "    def _perform_gradient_descent(\n",
    "            self,\n",
    "            Xa: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "            s_hat_parent: np.ndarray,\n",
    "            learning_rate_init: float,\n",
    "            n_gradient_descent_steps: int,\n",
    "            prior: np.ndarray) -> None:\n",
    "        weight_matrix = self._collect_weight_matrix()\n",
    "        momentum = 0\n",
    "        velocity = 0\n",
    "\n",
    "        if hasattr(self, 'callback') and hasattr(self, 'root'):\n",
    "            self.callback('gd_start', self.root, self)\n",
    "\n",
    "        for i in range(n_gradient_descent_steps):\n",
    "            gradient = self._compute_d_err_d_weights(Xa=Xa, y=y, prior=prior, s_hat_parent=s_hat_parent)\n",
    "\n",
    "            momentum = BETA_1_ADAM * momentum + (1 - BETA_1_ADAM) * gradient\n",
    "            velocity = BETA_2_ADAM * velocity + (1 - BETA_2_ADAM) * gradient**2\n",
    "            momentum_hat = momentum/(1 - BETA_1_ADAM**(i + 1))\n",
    "            velocity_hat = velocity/(1 - BETA_2_ADAM**(i + 1))\n",
    "\n",
    "            weight_matrix -= learning_rate_init * momentum_hat / (np.sqrt(velocity_hat) + EPS_ADAM)\n",
    "            self._distribute_weight_matrix(weight_matrix)\n",
    "\n",
    "        if hasattr(self, 'callback') and hasattr(self, 'root'):\n",
    "            self.callback('gd_end', self.root, self)\n",
    "\n",
    "    def update_s_hat(\n",
    "            self, Xa: np.ndarray,\n",
    "            s_hat_parent: Optional[np.ndarray]=None) -> None:\n",
    "        if s_hat_parent is None:\n",
    "            n_data = Xa.shape[0]\n",
    "            s_hat_parent = np.ones(n_data)\n",
    "\n",
    "        # compute left and right node outputs\n",
    "        self.s = sigmoid(Xa @ self.weights)\n",
    "        self.s_hat_left_ = s_hat_parent * self.s\n",
    "        self.s_hat_right = s_hat_parent * (1-self.s)\n",
    "\n",
    "        # recursively call children\n",
    "        if self.left__child is not None:\n",
    "            self.left__child.update_s_hat(Xa, self.s_hat_left_)\n",
    "\n",
    "        if self.right_child is not None:\n",
    "            self.right_child.update_s_hat(Xa, self.s_hat_right)\n",
    "\n",
    "    def _compute_d_err_d_weights(\n",
    "            self,\n",
    "            Xa: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "            prior: np.ndarray,\n",
    "            s_hat_parent: np.ndarray) -> np.ndarray:\n",
    "        # recursive forward pass\n",
    "        self.update_s_hat(Xa, s_hat_parent)\n",
    "\n",
    "        # recursive backward pass\n",
    "        class_idx = [y == i for i in range(len(np.unique(y)))]\n",
    "        d_k_d_s_hat = np.vstack([1*ci for ci in class_idx])\n",
    "\n",
    "        d_err_d_weights_list = []\n",
    "        self._compute_d_err_d_s_hat_and_collect_d_err_d_weights(\n",
    "            Xa, prior, s_hat_parent, class_idx, d_k_d_s_hat, d_err_d_weights_list)\n",
    "\n",
    "        return np.vstack(d_err_d_weights_list).T\n",
    "\n",
    "    def _compute_d_err_d_s_hat_and_collect_d_err_d_weights(\n",
    "            self,\n",
    "            Xa: np.ndarray,\n",
    "            prior: np.ndarray,\n",
    "            s_hat_parent: np.ndarray,\n",
    "            class_idx: List[np.ndarray],\n",
    "            d_k_d_s_hat: np.ndarray,\n",
    "            d_err_d_weights_list: List[np.ndarray]) -> np.ndarray:\n",
    "        # see node iteration order note at the top of this file\n",
    "\n",
    "        if self.left__child is None:\n",
    "            # leaf on the left\n",
    "            k_left_ = np.array([self.s_hat_left_[ci].sum() for ci in class_idx])\n",
    "            posterior_left_ = prior + k_left_\n",
    "            d_err_d_k_left_ = -d_log_multivariate_beta_d_alphas(posterior_left_)\n",
    "            d_err_d_s_hat_left_ = d_err_d_k_left_ @ d_k_d_s_hat\n",
    "        else:\n",
    "            # child node on the left\n",
    "            d_err_d_s_hat_left_ = self.left__child._compute_d_err_d_s_hat_and_collect_d_err_d_weights(\n",
    "                Xa=Xa,\n",
    "                prior=prior,\n",
    "                s_hat_parent=self.s_hat_left_,\n",
    "                class_idx=class_idx,\n",
    "                d_k_d_s_hat=d_k_d_s_hat,\n",
    "                d_err_d_weights_list=d_err_d_weights_list)\n",
    "\n",
    "        if self.right_child is None:\n",
    "            # leaf on the left\n",
    "            k_right = np.array([self.s_hat_right[ci].sum() for ci in class_idx])\n",
    "            posterior_right = prior + k_right\n",
    "            d_err_d_k_right = -d_log_multivariate_beta_d_alphas(posterior_right)\n",
    "            d_err_d_s_hat_right = d_err_d_k_right @ d_k_d_s_hat\n",
    "        else:\n",
    "            # child node on the left\n",
    "            d_err_d_s_hat_right = self.right_child._compute_d_err_d_s_hat_and_collect_d_err_d_weights(\n",
    "                Xa=Xa,\n",
    "                prior=prior,\n",
    "                s_hat_parent=self.s_hat_right,\n",
    "                class_idx=class_idx,\n",
    "                d_k_d_s_hat=d_k_d_s_hat,\n",
    "                d_err_d_weights_list=d_err_d_weights_list)\n",
    "\n",
    "        d_s_hat_left__ds = s_hat_parent\n",
    "        d_s_hat_right_ds = -s_hat_parent\n",
    "        d_err_d_s = d_err_d_s_hat_left_ * d_s_hat_left__ds + d_err_d_s_hat_right * d_s_hat_right_ds\n",
    "        d_err_d_s_hat_parent = d_err_d_s_hat_left_ * self.s + d_err_d_s_hat_right * (1-self.s)\n",
    "        d_s_d_a = self.s*(1-self.s)\n",
    "        d_err_d_weights = (d_err_d_s * d_s_d_a) @ Xa\n",
    "        d_err_d_weights_list.append(d_err_d_weights)\n",
    "\n",
    "        return d_err_d_s_hat_parent\n",
    "\n",
    "    def update_log_p_data_soft_split(\n",
    "            self,\n",
    "            Xa: np.ndarray,\n",
    "            y: np.ndarray,\n",
    "            s_hat_parent: np.ndarray,\n",
    "            prior: np.ndarray,\n",
    "            recursive: bool) -> None:\n",
    "        assert Xa.shape[0] == len(y)\n",
    "        assert s_hat_parent.ndim == 1\n",
    "\n",
    "        class_idx = [y == i for i in range(len(np.unique(y)))]\n",
    "\n",
    "        self.s = sigmoid(Xa @ self.weights)\n",
    "\n",
    "        self.s_hat_left_ = s_hat_parent * self.s\n",
    "        self.s_hat_right = s_hat_parent * (1 - self.s)\n",
    "\n",
    "        self.k_left_ = Node._compute_k(self.s_hat_left_, class_idx).T\n",
    "        self.k_right = Node._compute_k(self.s_hat_right, class_idx).T\n",
    "\n",
    "        betaln_prior = multivariate_betaln(prior)\n",
    "        self.log_p_data_left_ = compute_log_p_data(prior, self.k_left_, betaln_prior)\n",
    "        self.log_p_data_right = compute_log_p_data(prior, self.k_right, betaln_prior)\n",
    "\n",
    "        if recursive:\n",
    "            if self.left__child:\n",
    "                self.left__child.update_log_p_data_soft_split(Xa, y, self.s_hat_left_, prior, recursive)\n",
    "\n",
    "            if self.right_child:\n",
    "                self.right_child.update_log_p_data_soft_split(Xa, y, self.s_hat_right, prior, recursive)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_log_p_data_no_split(\n",
    "            y: np.ndarray,\n",
    "            s_hat_parent: np.ndarray,\n",
    "            prior: np.ndarray) -> np.ndarray:\n",
    "        class_idx = [y == i for i in range(len(np.unique(y)))]\n",
    "        k = Node._compute_k(s_hat_parent, class_idx).T\n",
    "        betaln_prior = multivariate_betaln(prior)\n",
    "        return compute_log_p_data(prior, k, betaln_prior)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_k(s_hat: np.ndarray, class_idx: List[np.array]) -> np.ndarray:\n",
    "        return np.array([s_hat[ci].sum(axis=0) for ci in class_idx])\n",
    "\n",
    "    def _collect_weight_matrix(self, weights_list: List[np.ndarray]=None) -> Optional[np.ndarray]:\n",
    "        # see node iteration order note at the top of this file\n",
    "\n",
    "        if weights_list is None:\n",
    "            weights_list = []\n",
    "            return_matrix = True\n",
    "        else:\n",
    "            return_matrix = False\n",
    "\n",
    "        if self.left__child is not None:\n",
    "            self.left__child._collect_weight_matrix(weights_list)\n",
    "\n",
    "        if self.right_child is not None:\n",
    "            self.right_child._collect_weight_matrix(weights_list)\n",
    "\n",
    "        weights_list.append(self.weights)\n",
    "\n",
    "        if return_matrix:\n",
    "            return np.array(weights_list).T\n",
    "\n",
    "    def _distribute_weight_matrix(self, weight_matrix: np.ndarray, index: Optional[int]=None) -> int:\n",
    "        # see node iteration order note at the top of this file\n",
    "\n",
    "        if index is None:\n",
    "            index = 0\n",
    "\n",
    "        if self.left__child is not None:\n",
    "            index = self.left__child._distribute_weight_matrix(weight_matrix, index)\n",
    "\n",
    "        if self.right_child is not None:\n",
    "            index = self.right_child._distribute_weight_matrix(weight_matrix, index)\n",
    "\n",
    "        self.weights = weight_matrix[:, index]\n",
    "        index += 1\n",
    "\n",
    "        # now that the weights have changed the node is potentially splittable once again\n",
    "        self.left__is_splittable = True\n",
    "        self.right_is_splittable = True\n",
    "\n",
    "        return index\n",
    "\n",
    "    def update_depth(self, depth: int) -> int:\n",
    "        depth = max(depth, self.level+1)\n",
    "\n",
    "        if self.left__child is not None:\n",
    "            depth = self.left__child.update_depth(depth)\n",
    "\n",
    "        if self.right_child is not None:\n",
    "            depth = self.right_child.update_depth(depth)\n",
    "\n",
    "        return depth\n",
    "\n",
    "    def update_n_leaves(self, n_leaves: int) -> int:\n",
    "        if self.left__child is None:\n",
    "            n_leaves += 1\n",
    "        else:\n",
    "            n_leaves = self.left__child.update_n_leaves(n_leaves)\n",
    "\n",
    "        if self.right_child is None:\n",
    "            n_leaves += 1\n",
    "        else:\n",
    "            n_leaves = self.right_child.update_n_leaves(n_leaves)\n",
    "\n",
    "        return n_leaves\n",
    "\n",
    "    def update_feature_importance(self, feature_importance: np.ndarray) -> None:\n",
    "        # the more the normal vector is oriented along a given dimension's axis the\n",
    "        # more important that dimension is, so weight the gain in log-likelihood by\n",
    "        # the absolute value of the unit hyperplane normal\n",
    "        log_p_gain = self.log_p_data_split - self.log_p_data_no_split\n",
    "        hyperplane_normal = self.weights[1:] / np.linalg.norm(self.weights[1:])\n",
    "        feature_importance += log_p_gain * np.abs(hyperplane_normal)\n",
    "\n",
    "        if self.left__child is not None:\n",
    "            self.left__child.update_feature_importance(feature_importance)\n",
    "\n",
    "        if self.right_child is not None:\n",
    "            self.right_child.update_feature_importance(feature_importance)\n",
    "\n",
    "    def predict_proba_leaf_left_(self) -> np.ndarray:\n",
    "        if self.posterior_left_ is None:\n",
    "            return np.nan\n",
    "\n",
    "        return self.posterior_left_ / self.posterior_left_.sum()\n",
    "\n",
    "    def predict_proba_leaf_right(self) -> np.ndarray:\n",
    "        if self.posterior_right is None:\n",
    "            return np.nan\n",
    "\n",
    "        return self.posterior_right / self.posterior_right.sum()\n",
    "\n",
    "    def get_n_data(self) -> float:\n",
    "        return self.k_left_.sum() + self.k_right.sum()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self._str('', '')\n",
    "\n",
    "    def _str(self, prefix: str, children_prefix: str) -> str:\n",
    "        s = prefix\n",
    "\n",
    "        # choose hyperplane origin to be the closest point to the origin\n",
    "        weights = self.weights\n",
    "        normal = weights[1:]\n",
    "        origin = normal * -weights[0] / np.dot(normal, normal)\n",
    "        origin_str = np.array2string(origin, max_line_width=9999, separator=', ', floatmode='maxprec_equal')\n",
    "        normal_str = np.array2string(normal, max_line_width=9999, separator=', ', floatmode='maxprec_equal')\n",
    "        s += f'origin={origin_str}, normal={normal_str}'\n",
    "        s += '\\n'\n",
    "\n",
    "        child_prefix_left_ = children_prefix + '├─ left: '\n",
    "        child_prefix_right = children_prefix + '└─ right:'\n",
    "        if self.left__child is not None:\n",
    "            s += self.left__child._str(child_prefix_left_, children_prefix + '│  ')\n",
    "        else:\n",
    "            s += f'{child_prefix_left_} p(y)={self.predict_proba_leaf_left_()}, n={self.k_left_.sum()}\\n'\n",
    "\n",
    "        if self.right_child is not None:\n",
    "            s += self.right_child._str(child_prefix_right, children_prefix + '   ')\n",
    "        else:\n",
    "            s += f'{child_prefix_right} p(y)={self.predict_proba_leaf_right()}, n={self.k_right.sum()}\\n'\n",
    "\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types\n",
    "INPUT = Union[np.ndarray, pd.DataFrame, List[List[float]]]\n",
    "TARGET = Union[np.ndarray, List[float]]\n",
    "\n",
    "\n",
    "class AdaptiveBayesianReticulum(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    The Adaptive Bayesian Reticulum classification model for binary and\n",
    "    multiclass classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    prior : tuple, shape = [n_classes], optional, default=(number of instances of each class divided by 20)\n",
    "        The hyperparameters (alpha_0, alpha_1, ..., alpha_[N-1]) of the Dirichlet\n",
    "        conjugate prior, see [1] and [2]. All alpha_i must be positive, where\n",
    "        alpha_i represents the number of prior pseudo-observations of class i.\n",
    "        Defaults to 1/20 times the number of observations in each class, i.e.,\n",
    "        5 % prior strength. If the problem is binary classification then the\n",
    "        theDirichlet distribution collapses to a Beta distribution with an\n",
    "        (alpha, beta) prior.\n",
    "\n",
    "    pruning_factor : float, optional, default=1.05\n",
    "        The factor by which the log likelihood of a split has to be larger than\n",
    "        the log likelihood of no split in order to keep the split during the\n",
    "        pruning stage. Larger values lead to more aggressive pruning and may\n",
    "        lead to underfitting, while small values encourage model complexity and\n",
    "        may lead to overfitting. The value provided must be >= 1.\n",
    "\n",
    "    n_iter : int, default=50\n",
    "        The number of attempts to add new nodes by splitting\n",
    "        existing ones. The resulting tree will at most contain n_iter nodes, but\n",
    "        it may contain less due to pruning. Larger values lead to\n",
    "        more complex and expressive models at the cost of potentially overfitting\n",
    "        and slower training performance, while smaller values restrict model\n",
    "        complexity and training time but can lead to underfitting.\n",
    "\n",
    "    learning_rate_init : float, optional, default=1e-4\n",
    "        The initial gradient descent learning rate. We use the adaptive 'Adam'\n",
    "        gradient descent method, see [3] and [4].\n",
    "\n",
    "    n_gradient_descent_steps : int, optional, default=100\n",
    "        The number of gradient descent steps to perform in each iteration. Half\n",
    "        of the steps will perform local gradient descent on only the newly added\n",
    "        node and half will be applied during global gradient descent involving\n",
    "        the whole tree.\n",
    "\n",
    "    initial_relative_stiffness : float\n",
    "        The initial stiffness of the problem, see the paper for an explanation of\n",
    "        'stiffness'. Small values (0.1...5) represent \"soft\" splits allowing for\n",
    "        further optimization through gradient descent whereas large values (> 20)\n",
    "        effectively disable further progress through gradient descent because\n",
    "        the split is already close to a Heaviside step function with almost zero\n",
    "        gradients almost everywhere. While this sounds like a disadvantage it can\n",
    "        actually be advantageous to model performance if the data has a complex\n",
    "        structure that soft splits don't capture well.\n",
    "\n",
    "    random_state : int, default=666\n",
    "        The initial random state to be set in numpy.random.seed().\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "\n",
    "    .. [1] https://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical/multinomial\n",
    "    .. [2] https://en.wikipedia.org/wiki/Conjugate_prior#Discrete_distributions\n",
    "    .. [3] https://arxiv.org/abs/1412.6980\n",
    "    .. [4] https://ruder.io/optimizing-gradient-descent/\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            prior: Optional[Tuple[float, ...]]=None,\n",
    "            pruning_factor: float=1.05,\n",
    "            n_iter: int=50,\n",
    "            learning_rate_init: float=1e-4,\n",
    "            n_gradient_descent_steps: int=100,\n",
    "            initial_relative_stiffness: float=10,\n",
    "            random_state: int=666) -> None:\n",
    "        self.prior = prior\n",
    "        self.pruning_factor = pruning_factor\n",
    "        self.n_iter = n_iter\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.n_gradient_descent_steps = n_gradient_descent_steps\n",
    "        self.initial_relative_stiffness = initial_relative_stiffness\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(\n",
    "            self,\n",
    "            X: INPUT,\n",
    "            y: TARGET,\n",
    "            verbose: bool=False) -> AdaptiveBayesianReticulum:\n",
    "        \"\"\"\n",
    "        Trains this Adaptive Bayesian Reticulum classification model using the training set (X, y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or pandas.DataFrame, shape = [n_samples, n_features]\n",
    "            The training input samples.\n",
    "\n",
    "        y : array-like, shape = [n_samples]\n",
    "            The target values. Only the integers 0 and 1 are permitted.\n",
    "\n",
    "        verbose : bool\n",
    "            Prints training progress statements to the standard output.\n",
    "        \"\"\"\n",
    "\n",
    "        # validation and input transformation\n",
    "        X, y = check_X_y(X, y)\n",
    "        if type_of_target(y) not in ('binary', 'multiclass'):\n",
    "            raise ValueError(f'Unknown label type: {type_of_target(y)}')\n",
    "\n",
    "        y_universe, y_encoded = np.unique(y, return_inverse=True)\n",
    "        if not np.all(y == y_encoded):\n",
    "            y = y_encoded\n",
    "            self.classes_ = y_universe\n",
    "        else:\n",
    "            self.classes_ = None\n",
    "\n",
    "        y = self._ensure_float64(y)\n",
    "\n",
    "        unique = np.unique(y)\n",
    "\n",
    "        prior = self.prior\n",
    "        if prior is None:\n",
    "            prior = [len(np.where(y == yi)[0])/20 for yi in unique]\n",
    "\n",
    "        prior = np.asarray(prior)\n",
    "\n",
    "        n_classes = len(prior)\n",
    "        if len(unique) == 1:\n",
    "            raise ValueError('Classifier can\\'t train when only one class is present.')\n",
    "\n",
    "        if not np.all(unique == np.arange(0, n_classes)):\n",
    "            raise ValueError(f'Expected target values 0..{n_classes-1} but found {y.min()}..{y.max()}')\n",
    "\n",
    "        X = self._normalize_data(X)\n",
    "        n_data = X.shape[0]\n",
    "        assert n_data > 1\n",
    "\n",
    "        self.n_dim_ = X.shape[1]\n",
    "        if self.n_dim_ < 2:\n",
    "            raise ValueError(f'X has {self.n_dim_} feature(s) (shape={X.shape}) while a minimum of 2 is required.')\n",
    "\n",
    "        if n_data != len(y):\n",
    "            raise ValueError(f'Invalid shapes: X={X.shape}, y={y.shape}')\n",
    "\n",
    "        if self.pruning_factor < 1:\n",
    "            raise ValueError('The pruning_factor must be >= 1')\n",
    "\n",
    "        # initialize\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        n_gradient_descent_steps_local = self.n_gradient_descent_steps//2\n",
    "        n_gradient_descent_steps_global = self.n_gradient_descent_steps//2\n",
    "\n",
    "        # augment data matrix with a row of 1's corresponding to the bias/offset\n",
    "        Xa = np.hstack([np.ones((n_data, 1)), X])    # [n * n_dim'] (where n_dim' = n_dim+1 for the bias)\n",
    "\n",
    "        # create root node and train recursively\n",
    "        if verbose:\n",
    "            print(f'Creating root node at level=0, n_data={n_data}')\n",
    "\n",
    "        root = Node(level=0)\n",
    "\n",
    "        if hasattr(self, 'callback'):\n",
    "            root.root = root\n",
    "            root.callback = self.callback\n",
    "\n",
    "        # fit\n",
    "        root.try_fit(\n",
    "            Xa=Xa,\n",
    "            y=y,\n",
    "            prior=prior,\n",
    "            s_hat_parent=None,\n",
    "            learning_rate_init=self.learning_rate_init,\n",
    "            n_gradient_descent_steps=n_gradient_descent_steps_local,\n",
    "            initial_relative_stiffness=self.initial_relative_stiffness)\n",
    "\n",
    "        # grow tree\n",
    "        overall_proba = None  # type: Optional[np.ndarray]\n",
    "        for k in range(self.n_iter):\n",
    "            # determine terminal nodes\n",
    "            terminal_nodes_left_ = root.collect_terminal_nodes(is_left=True)\n",
    "            terminal_nodes_right = root.collect_terminal_nodes(is_left=False)\n",
    "\n",
    "            # get log-likelihood of all leaves, see 'unexplained potential' in the paper\n",
    "            log_p_data_left_ = np.array([node.log_p_data_left_ for node in terminal_nodes_left_])\n",
    "            log_p_data_right = np.array([node.log_p_data_right for node in terminal_nodes_right])\n",
    "\n",
    "            # choose node to split from all terminal nodes proportional to their (absolute) log-likelihood\n",
    "            relative_probabilities = np.concatenate([log_p_data_left_, log_p_data_right])\n",
    "            idx = pick_proportional(relative_probabilities)\n",
    "            if idx < len(terminal_nodes_left_):\n",
    "                node_to_split = terminal_nodes_left_[idx]\n",
    "                split_left = True\n",
    "            else:\n",
    "                node_to_split = terminal_nodes_right[idx - len(terminal_nodes_left_)]\n",
    "                split_left = False\n",
    "\n",
    "            # create new node\n",
    "            new_node = Node(level=1+node_to_split.level)\n",
    "            if split_left:\n",
    "                node_to_split.left__child = new_node\n",
    "            else:\n",
    "                node_to_split.right_child = new_node\n",
    "\n",
    "            if hasattr(self, 'callback'):\n",
    "                new_node.callback = self.callback\n",
    "                new_node.root = root\n",
    "\n",
    "            if verbose:\n",
    "                n_data = node_to_split.get_n_data()\n",
    "                child_side = 'left ' if split_left else 'right'\n",
    "                print(f'Splitting {child_side} child of node at level={node_to_split.level}, n_data={n_data:.2f}')\n",
    "\n",
    "            # try to fit the new node\n",
    "            is_fitted = new_node.try_fit(\n",
    "                Xa=Xa,\n",
    "                y=y,\n",
    "                prior=prior,\n",
    "                s_hat_parent=node_to_split.s_hat_left_ if split_left else node_to_split.s_hat_right,\n",
    "                learning_rate_init=self.learning_rate_init,\n",
    "                n_gradient_descent_steps=n_gradient_descent_steps_local,\n",
    "                initial_relative_stiffness=self.initial_relative_stiffness)\n",
    "\n",
    "            # check if new node is sensible\n",
    "            if is_fitted:\n",
    "                # keep node and perform global optimization\n",
    "                root.try_fit(\n",
    "                    Xa=Xa,\n",
    "                    y=y,\n",
    "                    prior=prior,\n",
    "                    s_hat_parent=None,\n",
    "                    learning_rate_init=self.learning_rate_init,\n",
    "                    n_gradient_descent_steps=n_gradient_descent_steps_global,\n",
    "                    initial_relative_stiffness=self.initial_relative_stiffness)\n",
    "\n",
    "                # prune splits that aren't sensible anymore\n",
    "                root_pruned, any_pruned = root.try_prune(\n",
    "                    Xa=Xa,\n",
    "                    y=y,\n",
    "                    prior=prior,\n",
    "                    pruning_factor=self.pruning_factor,\n",
    "                    verbose=verbose)\n",
    "\n",
    "                if hasattr(self, 'callback') and (root_pruned or any_pruned):\n",
    "                    self.callback('prune', root, self)\n",
    "\n",
    "                if root_pruned:\n",
    "                    if verbose:\n",
    "                        print('Pruned root node, no split left')\n",
    "\n",
    "                    root = None\n",
    "                    _, counts = np.unique(y, return_counts=True)\n",
    "                    overall_proba = counts/len(y)\n",
    "                    break\n",
    "            else:\n",
    "                # discard new node\n",
    "                if split_left:\n",
    "                    node_to_split.left__child = None\n",
    "                else:\n",
    "                    node_to_split.right_child = None\n",
    "\n",
    "        self.root_ = root\n",
    "        if overall_proba is not None:\n",
    "            self.overall_proba_ = overall_proba\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: INPUT) -> np.ndarray:\n",
    "        \"\"\"Predicts the class labels for each input in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or pandas.DataFrame, shape = [n_samples, n_features]\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array, shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "\n",
    "        return self._predict(X, predict_class=True)\n",
    "\n",
    "    def predict_proba(self, X: INPUT) -> np.ndarray:\n",
    "        \"\"\"Predict class probabilities of the input samples X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or pandas.DataFrame, shape = [n_samples, n_features]\n",
    "            The input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        p : array, shape = [n_samples, n_classes]\n",
    "            The predicted class probabilities.\n",
    "        \"\"\"\n",
    "\n",
    "        return self._predict(X, predict_class=False)\n",
    "\n",
    "    def _predict(self, X: INPUT, predict_class: bool) -> np.ndarray:\n",
    "        # input transformation and checks\n",
    "        X = check_array(X)  # type: np.ndarray\n",
    "        X = self._normalize_data(X)\n",
    "        self._ensure_is_fitted_and_valid(X)\n",
    "\n",
    "        Xa = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        n_data = Xa.shape[0]\n",
    "\n",
    "        if self.root_ is None:\n",
    "            if predict_class:\n",
    "                return np.argmax(self.overall_proba_) * np.ones(n_data)\n",
    "            else:\n",
    "                return self.overall_proba_ * np.ones((n_data, len(self.overall_proba_)))\n",
    "\n",
    "        self.root_.update_s_hat(Xa)\n",
    "        n_classes = len(self.root_.posterior_left_)\n",
    "        p = np.zeros((n_data, n_classes))\n",
    "        self.root_.update_probability(Xa, p)\n",
    "\n",
    "        assert np.all((p >= 0) & (p <= 1))\n",
    "        if predict_class:\n",
    "            cls = np.argmax(p, axis=1)\n",
    "            return cls if self.classes_ is None else self.classes_[cls]\n",
    "        else:\n",
    "            return p\n",
    "\n",
    "    def get_depth(self) -> int:\n",
    "        \"\"\"Computes and returns the tree depth.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        depth : int\n",
    "            The tree depth.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self._is_fitted() or self.root_ is None:\n",
    "            return 0\n",
    "\n",
    "        return self.root_.update_depth(0)\n",
    "\n",
    "    def get_n_leaves(self) -> int:\n",
    "        \"\"\"Computes and returns the total number of leaves of this tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        n_leaves : int\n",
    "            The number of leaves.\n",
    "        \"\"\"\n",
    "\n",
    "        if not self._is_fitted() or self.root_ is None:\n",
    "            return 0\n",
    "\n",
    "        return self.root_.update_n_leaves(0)\n",
    "\n",
    "    def feature_importance(self) -> np.ndarray:\n",
    "        \"\"\"Computes and returns the relative importance of the feature dimensions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_importance : array, shape = [number of features/dimensions]\n",
    "            The relative importance of each feature dimension.\n",
    "        \"\"\"\n",
    "\n",
    "        self._ensure_is_fitted_and_valid()\n",
    "\n",
    "        feature_importance = np.zeros(self.n_dim_)\n",
    "        self.root_.update_feature_importance(feature_importance)\n",
    "        feature_importance /= feature_importance.sum()\n",
    "\n",
    "        return feature_importance\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_data(X: INPUT) -> np.ndarray:\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        else:\n",
    "            if isinstance(X, list):\n",
    "                X = np.array(X)\n",
    "            elif np.isscalar(X):\n",
    "                X = np.array([X])\n",
    "\n",
    "            if X.ndim == 1:\n",
    "                X = np.expand_dims(X, 0)\n",
    "\n",
    "        X = AdaptiveBayesianReticulum._ensure_float64(X)\n",
    "\n",
    "        if X.ndim != 2:\n",
    "            raise ValueError(f'X should have 2 dimensions but has {X.ndim}')\n",
    "\n",
    "        return X\n",
    "\n",
    "    def _ensure_is_fitted_and_valid(self, X: np.ndarray=None) -> None:\n",
    "        if not self._is_fitted():\n",
    "            raise NotFittedError('Cannot predict on an untrained model; call .fit() first')\n",
    "\n",
    "        if X is not None and X.shape[1] != self.n_dim_:\n",
    "            raise ValueError(f'Bad input dimensions: Expected {self.n_dim_}, got {X.shape[1]}')\n",
    "\n",
    "    def _is_fitted(self) -> bool:\n",
    "        return hasattr(self, 'root_')\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_float64(data: np.ndarray) -> np.ndarray:\n",
    "        # check data types\n",
    "        if data.dtype in (\n",
    "                np.int8, np.int16, np.int32, np.int64,\n",
    "                np.uint8, np.uint16, np.uint32, np.uint64,\n",
    "                np.float32, np.float64):\n",
    "            return data\n",
    "\n",
    "        # check that data isn't complex\n",
    "        if np.any(np.iscomplex(data)):\n",
    "            raise ValueError('Complex data not supported')\n",
    "\n",
    "        # convert to np.float64 for performance reasons (matrices with floats but of type object are very slow)\n",
    "        data_float64 = data.astype(np.float64)\n",
    "        if not np.all(data == data_float64):\n",
    "            raise ValueError('Cannot convert data matrix to np.float64 without loss of precision. Please check your data.')\n",
    "\n",
    "        return data_float64\n",
    "\n",
    "    def __repr__(self, N_CHAR_MAX=700) -> str:\n",
    "        return str(self)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if not self._is_fitted():\n",
    "            return 'Unfitted model'\n",
    "\n",
    "        if self.root_ is None:\n",
    "            return 'Empty model'\n",
    "\n",
    "        return str(self.root_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scrape(url: str, proxies: Dict[str, str]) -> str:\n",
    "    return requests.get(url, proxies=proxies).text\n",
    "\n",
    "def load_ripley(proxies: Dict[str, str]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # load Ripley's synthetic dataset\n",
    "    def parse_ripley(text: str) -> np.ndarray:\n",
    "        lines = text.split('\\n')[1:]\n",
    "        return np.vstack([np.fromstring(lines[i], sep=' ') for i in range(len(lines)-1)])\n",
    "\n",
    "    train = parse_ripley(_scrape('https://www.stats.ox.ac.uk/pub/PRNN/synth.tr', proxies))\n",
    "    test = parse_ripley(_scrape('https://www.stats.ox.ac.uk/pub/PRNN/synth.te', proxies))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_hyperplane(\n",
    "        model: BaseEstimator,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        info_train: str,\n",
    "        X_test: np.ndarray,\n",
    "        y_test: np.ndarray,\n",
    "        info_test: str) -> None:\n",
    "    n_classes = int(np.max(y_train)) + 1\n",
    "    if n_classes != 2:\n",
    "        print('Warning: Cannot plot more than 2 classes')\n",
    "        return\n",
    "\n",
    "    cmap = plt.get_cmap('coolwarm')\n",
    "\n",
    "    x_min = min(X_train[:, 0].min(), X_test[:, 0].min())\n",
    "    x_max = max(X_train[:, 0].max(), X_test[:, 0].max())\n",
    "    y_min = min(X_train[:, 1].min(), X_test[:, 1].min())\n",
    "    y_max = max(X_train[:, 1].max(), X_test[:, 1].max())\n",
    "\n",
    "    x_lim = [0, x_max+0.2*(x_max-x_min)]\n",
    "    y_lim = [y_min-0.2*(y_max-y_min), y_max+0.2*(y_max-y_min)]\n",
    "\n",
    "    def plot(X: np.ndarray, y: np.ndarray, info: str, cmap: Colormap) -> None:\n",
    "        for i in range(n_classes):\n",
    "            class_i = y == i\n",
    "            plt.plot(\n",
    "                X[np.where(class_i)[0], 0],\n",
    "                X[np.where(class_i)[0], 1],\n",
    "                'o',\n",
    "                ms=4,\n",
    "                c=cmap(i/(n_classes-1)),\n",
    "                label=f'Class {i}')\n",
    "\n",
    "        draw_node_2d_hyperplane(model, x_lim, y_lim, cmap)\n",
    "\n",
    "        plt.title(info)\n",
    "        plt.xlabel('Trade Size - Notional')\n",
    "        plt.ylabel('Delta Mid')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.figure(figsize=[10, 16])\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plot(X_train, y_train, info_train, cmap)\n",
    "    plt.xlim(x_lim)\n",
    "    plt.ylim(y_lim)\n",
    "#     plt.gca().set_aspect(1)\n",
    "    plt.gca().set_aspect('auto')\n",
    "    \n",
    "    plt.subplot(212)\n",
    "    plot(X_test, y_test, info_test, cmap)\n",
    "    plt.xlim(x_lim)\n",
    "    plt.ylim(y_lim)\n",
    "#     plt.gca().set_aspect(1)\n",
    "    plt.gca().set_aspect('auto')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_node_2d_hyperplane(model: Any, x_lim: List[float], y_lim: List[float], cmap: Colormap) -> None:\n",
    "    n = 100\n",
    "    x = np.linspace(x_lim[0], x_lim[1], n)\n",
    "    y = np.linspace(y_lim[0], y_lim[1], n)\n",
    "    print(x_lim,y_lim)\n",
    "    xg, yg = np.meshgrid(x, y)\n",
    "    X = np.array([xg.flatten(), yg.flatten()]).T\n",
    "    p = model.predict_proba(X)[:, 1]\n",
    "    p = p.reshape(xg.shape)\n",
    "\n",
    "    levels = np.linspace(0, 1, 11)\n",
    "    cf = plt.contourf(xg, yg, p, levels=levels, alpha=0.8, cmap=cmap, zorder=1, vmin=0, vmax=1)\n",
    "    plt.colorbar(cf, ticks=levels, fraction=0.03, pad=0.04)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_ripley(None)\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1]\n",
    "\n",
    "# train m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating root node at level=0, n_data=1991\n",
      "Splitting right child of node at level=0, n_data=1991.00\n",
      "Pruning node at level 1\n",
      "Pruning node at level 0\n",
      "Pruned root node, no split left\n",
      "Model:\n",
      "Empty model\n",
      "Training took 0:00:00.117673\n",
      "Depth:  0\n",
      "Leaves: 0\n",
      "Train: Log-loss = 0.682129968117937, accuracy = 57.4083 %\n",
      "Test: Log-loss = 0.6918314209979913, accuracy = 54.1586 %\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_feature_importance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-310-0a0323cf1135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Feature importance: {model.feature_importance()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c8d1bc14252a>\u001b[0m in \u001b[0;36mfeature_importance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mfeature_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dim_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_feature_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_importance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mfeature_importance\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mfeature_importance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'update_feature_importance'"
     ]
    }
   ],
   "source": [
    "model = AdaptiveBayesianReticulum(\n",
    "    prior=(1, 1),\n",
    "    pruning_factor=1.05,\n",
    "    n_iter=40,\n",
    "    learning_rate_init=0.1,\n",
    "    n_gradient_descent_steps=100,\n",
    "    initial_relative_stiffness=2,\n",
    "    random_state=42)\n",
    "\n",
    "t0 = dt.datetime.utcnow()\n",
    "model.fit(X_train, y_train, verbose=True)\n",
    "t1 = dt.datetime.utcnow()\n",
    "print('Model:')\n",
    "print(model)\n",
    "print(f'Training took {t1-t0}')\n",
    "\n",
    "# evaluate performance\n",
    "log_loss_train = log_loss(y_train, model.predict_proba(X_train))\n",
    "log_loss_test = log_loss(y_test, model.predict_proba(X_test))\n",
    "accuracy_train = accuracy_score(y_train, model.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, model.predict(X_test))\n",
    "info_train = f'Train: Log-loss = {log_loss_train}, accuracy = {100*accuracy_train:.4f} %'\n",
    "info_test = f'Test: Log-loss = {log_loss_test}, accuracy = {100*accuracy_test:.4f} %'\n",
    "print(f'Depth:  {model.get_depth()}')\n",
    "print(f'Leaves: {model.get_n_leaves()}')\n",
    "print(info_train)\n",
    "print(info_test)\n",
    "print(f'Feature importance: {model.feature_importance()}')\n",
    "\n",
    "# plot\n",
    "plot_2d_hyperplane(model, X_train, y_train, info_train, X_test, y_test, info_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"/Users/krishna/Downloads/RfqData.xlsx\"\n",
    "df = pd.read_excel(data)\n",
    "test_df = pd.read_excel(data,sheet_name = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Bond</th>\n",
       "      <th>Side</th>\n",
       "      <th>Notional</th>\n",
       "      <th>Counterparty</th>\n",
       "      <th>MidPrice</th>\n",
       "      <th>QuotedPrice</th>\n",
       "      <th>Competitors</th>\n",
       "      <th>Traded</th>\n",
       "      <th>NextMidPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000</td>\n",
       "      <td>Bond_2</td>\n",
       "      <td>Offer</td>\n",
       "      <td>10000000</td>\n",
       "      <td>Ctpy_0</td>\n",
       "      <td>124.01</td>\n",
       "      <td>124.25</td>\n",
       "      <td>1</td>\n",
       "      <td>MISSED</td>\n",
       "      <td>124.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25001</td>\n",
       "      <td>Bond_0</td>\n",
       "      <td>Bid</td>\n",
       "      <td>1000</td>\n",
       "      <td>Ctpy_1</td>\n",
       "      <td>98.07</td>\n",
       "      <td>98.06</td>\n",
       "      <td>1</td>\n",
       "      <td>DONE</td>\n",
       "      <td>98.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25002</td>\n",
       "      <td>Bond_1</td>\n",
       "      <td>Offer</td>\n",
       "      <td>1000</td>\n",
       "      <td>Ctpy_1</td>\n",
       "      <td>170.30</td>\n",
       "      <td>170.40</td>\n",
       "      <td>1</td>\n",
       "      <td>MISSED</td>\n",
       "      <td>170.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25003</td>\n",
       "      <td>Bond_0</td>\n",
       "      <td>Bid</td>\n",
       "      <td>20000</td>\n",
       "      <td>Ctpy_0</td>\n",
       "      <td>98.00</td>\n",
       "      <td>97.98</td>\n",
       "      <td>4</td>\n",
       "      <td>DONE</td>\n",
       "      <td>97.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25004</td>\n",
       "      <td>Bond_1</td>\n",
       "      <td>Offer</td>\n",
       "      <td>1000</td>\n",
       "      <td>Ctpy_3</td>\n",
       "      <td>171.12</td>\n",
       "      <td>171.16</td>\n",
       "      <td>2</td>\n",
       "      <td>DONE</td>\n",
       "      <td>171.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time    Bond   Side  Notional Counterparty  MidPrice  QuotedPrice  \\\n",
       "0  25000  Bond_2  Offer  10000000       Ctpy_0    124.01       124.25   \n",
       "1  25001  Bond_0    Bid      1000       Ctpy_1     98.07        98.06   \n",
       "2  25002  Bond_1  Offer      1000       Ctpy_1    170.30       170.40   \n",
       "3  25003  Bond_0    Bid     20000       Ctpy_0     98.00        97.98   \n",
       "4  25004  Bond_1  Offer      1000       Ctpy_3    171.12       171.16   \n",
       "\n",
       "   Competitors  Traded  NextMidPrice  \n",
       "0            1  MISSED        124.24  \n",
       "1            1    DONE         98.08  \n",
       "2            1  MISSED        170.64  \n",
       "3            4    DONE         97.94  \n",
       "4            2    DONE        171.46  "
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ctpy_2': 0.1623931623931624,\n",
       " 'Ctpy_1': 0.5597862391449566,\n",
       " 'Ctpy_3': 0.6148678907299597,\n",
       " 'Ctpy_0': 0.4922779922779923}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_parties = set(df['Counterparty'])\n",
    "hit_ratio_party = {}\n",
    "for party in counter_parties:\n",
    "    temp_df = df[df['Counterparty']==party]\n",
    "    temp_done_df = temp_df[temp_df[\"Traded\"] == \"DONE\"]\n",
    "    hit_ratio_party[party] = len(temp_done_df)/len(temp_df)\n",
    "hit_ratio_party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({\"Counterparty\":hit_ratio_party})\n",
    "# new_df = new_df.replace({\"Bond\":hit_ratio_bond})\n",
    "test_df = test_df.replace({\"Counterparty\":hit_ratio_party})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rfq_params(df):\n",
    "    df = df.merge(pd.get_dummies(df[[\"Time\",\"Bond\"]]))\n",
    "    df = df.drop([\"Bond\",\"Time\",\"NextMidPrice\"],axis=1)\n",
    "    \n",
    "    bid_df = df[df['Side']==\"Bid\"]\n",
    "#     notional = np.array(bid_df[\"Notional\"])\n",
    "#     delta_mid = np.array(bid_df[\"MidPrice\"] - bid_df[\"QuotedPrice\"])\n",
    "#     num_competitors = np.array(bid_df[\"Competitors\"])\n",
    "#     extra = np.array(bid_df.iloc[: , -7:])    \n",
    "    bid_df[\"delta_mid\"] = bid_df[\"MidPrice\"] - bid_df[\"QuotedPrice\"]\n",
    "    ys = np.array((bid_df[\"Traded\"] == \"DONE\").astype(int))\n",
    "    bid_df = bid_df.drop([\"Side\",'Traded',\"MidPrice\",\"QuotedPrice\"],axis = 1)\n",
    "\n",
    "    bid_df[\"Label\"] = ys\n",
    "    return bid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(df,train_size=0.75):\n",
    "    df = get_rfq_params(df)\n",
    "    print(df.columns)\n",
    "#     data = np.array([notional,num_competitors,extra.flatten(),delta_mid,ys]).T\n",
    "    data = np.array(df)\n",
    "    n = len(data)\n",
    "    train = data[:int(train_size*n)]\n",
    "    test = data[int(train_size*n):]\n",
    "    return train, test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Notional', 'Counterparty', 'Competitors', 'Bond_Bond_0', 'Bond_Bond_1',\n",
      "       'Bond_Bond_2', 'delta_mid', 'Label'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-303-cfa3f7a6fa11>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bid_df[\"delta_mid\"] = bid_df[\"MidPrice\"] - bid_df[\"QuotedPrice\"]\n"
     ]
    }
   ],
   "source": [
    "train,test = get_train_test_data(df,0.8)\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+03, 5.59786239e-01, 1.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e-02],\n",
       "       [2.00000000e+04, 4.92277992e-01, 4.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 2.00000000e-02],\n",
       "       [1.00000000e+03, 5.59786239e-01, 3.00000000e+00, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 3.00000000e-02],\n",
       "       ...,\n",
       "       [1.00000000e+03, 6.14867891e-01, 1.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e-02],\n",
       "       [1.00000000e+03, 5.59786239e-01, 4.00000000e+00, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 7.00000000e-02],\n",
       "       [1.00000000e+03, 5.59786239e-01, 1.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e-02]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "origin=[ 8.83695355e-08, -3.81245755e-03,  1.09467665e-03,  5.14823136e-03, -1.58299687e-03, -7.91442938e-04,  8.89690694e-02], normal=[ 2.20779457e-05, -9.52491493e-01,  2.73490310e-01,  1.28621670e+00, -3.95490581e-01, -1.97731426e-01,  2.22277313e+01]\n",
      "├─ left: origin=[-2.19934658e-09,  1.35781041e-01,  7.52451557e-03,  1.63732145e-02,  5.29602091e-02,  7.48640362e-02, -2.72333686e-01], normal=[ 4.82740313e-08, -2.98029346e+00, -1.65157554e-01, -3.59379953e-01, -1.16243743e+00, -1.64321024e+00,  5.97752306e+00]\n",
      "│  ├─ left: origin=[-2.36774744e-10, -2.00050175e-04, -5.08973684e-04,  1.72733022e-03, -1.12252964e-02,  9.68915628e-03,  7.25051173e-02], normal=[-3.21819183e-08, -2.71903932e-02, -6.91786179e-02,  2.34775040e-01, -1.52571835e+00,  1.31692946e+00,  9.85474093e+00]\n",
      "│  │  ├─ left: origin=[-9.13611307e-10,  1.81635290e-01, -2.90785111e-02, -1.47695831e-02,  7.42669207e-02, -2.04650683e-02,  2.17009011e-01], normal=[ 2.19965865e-08, -4.37314680e+00,  7.00109531e-01,  3.55600253e-01, -1.78808946e+00,  4.92727751e-01, -5.22482313e+00]\n",
      "│  │  │  ├─ left:  p(y)=[0.6305509 0.3694491], n=196.18776725350511\n",
      "│  │  │  └─ right: p(y)=[0.79813892 0.20186108], n=79.21122976461592\n",
      "│  │  └─ right:origin=[ 4.38863655e-10, -4.28066777e-04, -5.90511594e-04,  6.21244615e-04,  9.69132085e-04,  6.01450461e-04, -3.65760702e-02], normal=[-3.50199870e-07,  3.41584289e-01,  4.71210320e-01, -4.95734338e-01, -7.73337975e-01, -4.79939204e-01,  2.91865933e+01]\n",
      "│  │     ├─ left:  p(y)=[0.5315082 0.4684918], n=206.93043556550523\n",
      "│  │     └─ right: p(y)=[0.29511594 0.70488406], n=56.29793073734237\n",
      "│  └─ right:origin=[-3.07257731e-09,  1.70823486e-01, -1.35940951e-02, -8.45748094e-02,  8.77693411e-02, -1.35204543e-02,  2.02193942e-02], normal=[-8.27762874e-08,  4.60204335e+00, -3.66229587e-01, -2.27847439e+00,  2.36453617e+00, -3.64245678e-01,  5.44717419e-01]\n",
      "│     ├─ left: origin=[ 3.17310020e-09,  5.25686029e-01,  1.67985963e-03, -2.26731434e-03, -1.18064309e-02, -5.79736082e-02, -1.66032129e-01], normal=[ 1.47629042e-07,  2.44576344e+01,  7.81557630e-02, -1.05487196e-01, -5.49296262e-01, -2.69723226e+00, -7.72467384e+00]\n",
      "│     │  ├─ left:  p(y)=[0.37431515 0.62568485], n=43.07120251337902\n",
      "│     │  └─ right: p(y)=[0.57872207 0.42127793], n=168.19915928228656\n",
      "│     └─ right:origin=[ 4.11789911e-10, -5.74616322e-03,  1.33481188e-03,  1.55446359e-03, -2.45356311e-03, -7.53684058e-04,  8.22947219e-02], normal=[-1.57889081e-07,  2.20320218e+00, -5.11795494e-01, -5.96014669e-01,  9.40748704e-01,  2.88978628e-01, -3.15535609e+01]\n",
      "│        ├─ left:  p(y)=[0.294402 0.705598], n=220.64634093473884\n",
      "│        └─ right: p(y)=[0.5764616 0.4235384], n=176.47160828195166\n",
      "└─ right:origin=[ 2.93919596e-07,  1.20379147e-01,  4.25642029e-03, -1.01222353e-03, -8.76654451e-03, -6.68457450e-03, -2.24471659e-01], normal=[-3.23826214e-05, -1.32627847e+01, -4.68951538e-01,  1.11521831e-01,  9.65854933e-01,  7.36473676e-01,  2.47311879e+01]\n",
      "   ├─ left: origin=[-5.36691072e-07,  1.18032456e-01, -7.40562032e-03, -5.26169877e-03,  1.84135510e-02,  8.96624877e-04, -1.72626690e-01], normal=[ 5.40419813e-05, -1.18852504e+01,  7.45707198e-01,  5.29825521e-01, -1.85414819e+00, -9.02854312e-02,  1.73826040e+01]\n",
      "   │  ├─ left: origin=[-7.93171553e-08, -1.36353429e-03, -1.33542092e-05,  2.45057375e-03,  1.47854247e-02,  2.92946555e-03,  5.96684614e-02], normal=[ 1.91756857e-05,  3.29647538e-01,  3.22850862e-03, -5.92449792e-01, -3.57451873e+00, -7.08226493e-01, -1.44254249e+01]\n",
      "   │  │  ├─ left:  p(y)=[0.41575199 0.58424801], n=91.67836294892602\n",
      "   │  │  └─ right:origin=[  1.29691198,   1.35454266,   2.08361637,  -0.40433619,  -1.24196238,  -1.46568986, -35.84651954], normal=[  0.59590762,   0.62238788,   0.95738408,  -0.18578517,  -0.57065927,  -0.67345801, -16.47082805]\n",
      "   │  │     ├─ left:  p(y)=[0.52021077 0.47978923], n=90.48669099922654\n",
      "   │  │     └─ right: p(y)=[0.75117845 0.24882155], n=32.865094850238194\n",
      "   │  └─ right:origin=[ 1.20274900e-01,  9.25553382e+00, -2.78281908e-01, -6.02645372e-02, -9.20118623e-03, -2.14480948e-01,  6.30398642e+00], normal=[ 3.96379102e-01,  3.05026252e+01, -9.17108500e-01, -1.98608381e-01, -3.03235167e-02, -7.06845449e-01,  2.07754775e+01]\n",
      "   │     ├─ left:  p(y)=[0.51378663 0.48621337], n=58.786551384189224\n",
      "   │     └─ right:origin=[  0.24402998,  -0.06654149,   0.67175223,   0.14940517,   0.75941854,   0.02604104, -15.62377964], normal=[  0.50634735,  -0.13806953,   1.39384499,   0.31000663,   1.57574725,   0.05403358, -32.41839185]\n",
      "   │        ├─ left:  p(y)=[0.16917595 0.83082405], n=43.17883168132849\n",
      "   │        └─ right: p(y)=[0.38561034 0.61438966], n=69.94088560110183\n",
      "   └─ right:origin=[ 3.04015580e-09, -3.25557443e-04,  9.38763575e-05,  8.75538589e-05, -2.60688442e-04,  1.72226583e-06,  1.55368994e-02], normal=[-2.02682786e-05,  2.17044434e+00, -6.25860086e-01, -5.83708903e-01,  1.73797211e+00, -1.14820970e-02, -1.03582259e+02]\n",
      "      ├─ left:  p(y)=[0.06130076 0.93869924], n=148.8269172083293\n",
      "      └─ right:origin=[-2.58952402e-09, -5.61061686e-05, -4.52068140e-05,  3.52359429e-05,  6.61391316e-05,  3.33558177e-05,  8.64160113e-03], normal=[-3.81180466e-05, -8.25888284e-01, -6.65448719e-01,  5.18676522e-01,  9.73574480e-01,  4.91000896e-01,  1.27205213e+02]\n",
      "         ├─ left:  p(y)=[0.39068907 0.60931093], n=167.104062445245\n",
      "         └─ right: p(y)=[0.05579629 0.94420371], n=141.11692854809073\n",
      "\n",
      "Training took 0:00:00.114387\n",
      "Depth:  5\n",
      "Leaves: 17\n",
      "Train: Log-loss = 0.5370494872711851, accuracy = 79.8594 %\n",
      "Test: Log-loss = 0.5600127579962841, accuracy = 84.5261 %\n",
      "Feature importance: [5.54845112e-04 1.46616959e-01 1.21066241e-02 2.29231754e-02\n",
      " 3.54588194e-02 2.39207304e-02 7.58418846e-01]\n"
     ]
    }
   ],
   "source": [
    "model = AdaptiveBayesianReticulum(\n",
    "    prior=(1, 1),\n",
    "    pruning_factor=1.,\n",
    "    n_iter=40,\n",
    "    learning_rate_init=0.1,\n",
    "    n_gradient_descent_steps=1,\n",
    "    initial_relative_stiffness=2,\n",
    "    random_state=42)\n",
    "\n",
    "t0 = dt.datetime.utcnow()\n",
    "model.fit(X_train, y_train, verbose=False)\n",
    "t1 = dt.datetime.utcnow()\n",
    "print('Model:')\n",
    "print(model)\n",
    "print(f'Training took {t1-t0}')\n",
    "\n",
    "# evaluate performance\n",
    "log_loss_train = log_loss(y_train, model.predict_proba(X_train))\n",
    "y_pred = model.predict_proba(X_train)\n",
    "\n",
    "log_loss_test = log_loss(y_test, model.predict_proba(X_test))\n",
    "accuracy_train = accuracy_score(y_train, model.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, model.predict(X_test))\n",
    "info_train = f'Train: Log-loss = {log_loss_train}, accuracy = {100*accuracy_train:.4f} %'\n",
    "info_test = f'Test: Log-loss = {log_loss_test}, accuracy = {100*accuracy_test:.4f} %'\n",
    "print(f'Depth:  {model.get_depth()}')\n",
    "print(f'Leaves: {model.get_n_leaves()}')\n",
    "print(info_train)\n",
    "print(info_test)\n",
    "print(f'Feature importance: {model.feature_importance()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 11999800.0] [0.07189821672580292, 0.7053628363973192]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Bad input dimensions: Expected 7, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-bb150943579b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_2d_hyperplane\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-4e7e5a8a397b>\u001b[0m in \u001b[0;36mplot_2d_hyperplane\u001b[0;34m(model, X_train, y_train, info_train, X_test, y_test, info_test)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m211\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_lim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4e7e5a8a397b>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(X, y, info, cmap)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 label=f'Class {i}')\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mdraw_node_2d_hyperplane\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_lim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4e7e5a8a397b>\u001b[0m in \u001b[0;36mdraw_node_2d_hyperplane\u001b[0;34m(model, x_lim, y_lim, cmap)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mxg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeshgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c8d1bc14252a>\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \"\"\"\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mINPUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_class\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c8d1bc14252a>\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X, predict_class)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: np.ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_is_fitted_and_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mXa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c8d1bc14252a>\u001b[0m in \u001b[0;36m_ensure_is_fitted_and_valid\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dim_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Bad input dimensions: Expected {self.n_dim_}, got {X.shape[1]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Bad input dimensions: Expected 7, got 2"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_2d_hyperplane(model, X_train, y_train, info_train, X_test, y_test, info_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Notional', 'Counterparty', 'Competitors', 'Bond_Bond_0', 'Bond_Bond_1',\n",
      "       'Bond_Bond_2', 'delta_mid', 'Label'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-303-cfa3f7a6fa11>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bid_df[\"delta_mid\"] = bid_df[\"MidPrice\"] - bid_df[\"QuotedPrice\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Test: Log-loss = 0.5600127579962841, accuracy = 84.5261 %'"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_,_ = get_train_test_data(test_df,1)\n",
    "X_test = test_[:, :-1]\n",
    "y_test = test_[:, -1]\n",
    "\n",
    "log_loss_test = log_loss(y_test, model.predict_proba(X_test))\n",
    "accuracy_test = accuracy_score(y_test, model.predict(X_test))\n",
    "info_test = f'Test: Log-loss = {log_loss_test}, accuracy = {100*accuracy_test:.4f} %'\n",
    "info_test\n",
    "# plot_2d_hyperplane(model, X_train, y_train, info_train, X_test, y_test, info_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
